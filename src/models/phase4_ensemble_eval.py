"""
phase4_ensemble_eval.py
=======================

Offline ensemble evaluator for Phase-4 UIDAI enrolment forecasting.

**PURPOSE:**
Combine XGBoost and LightGBM out-of-fold predictions using weighted
blending. Grid search over blend weights to find the optimal ensemble.

**PREREQUISITES:**
1. Run Phase-4 v3 XGBoost CV (produces OOF parquet)
2. Run LightGBM experiment (produces OOF parquet)

**USAGE:**
    python src/models/phase4_ensemble_eval.py

**OUTPUT:**
    artifacts/phase4_ensemble_metrics.json

Author: UIDAI Forecast Team
"""

from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Tuple

import numpy as np
import pandas as pd
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


# =============================================================================
# Configuration
# =============================================================================

# OOF prediction files (generated by respective CV runs)
XGB_OOF_PATH = Path("artifacts/xgb_phase4_v3_baseline_oof.parquet")
LGBM_OOF_PATH = Path("artifacts/phase4_lgbm_experiment_oof.parquet")

# Output path
ENSEMBLE_METRICS_PATH = Path("artifacts/phase4_ensemble_metrics.json")

# Blend weight search grid
BLEND_WEIGHTS = np.arange(0.0, 1.05, 0.05)  # 0, 0.05, 0.10, ..., 1.0


# =============================================================================
# Data Classes
# =============================================================================

@dataclass
class EnsembleResult:
    """Result from a single blend weight evaluation."""
    xgb_weight: float
    lgbm_weight: float
    r2: float
    mae: float
    rmse: float


# =============================================================================
# Ensemble Evaluator
# =============================================================================

class Phase4EnsembleEval:
    """
    Evaluate weighted ensemble of XGBoost and LightGBM OOF predictions.
    
    Uses a grid search over blend weights to find optimal combination.
    """
    
    def __init__(
        self,
        xgb_oof_path: Path = XGB_OOF_PATH,
        lgbm_oof_path: Path = LGBM_OOF_PATH,
        target_col: str = "total_enrolment",
        xgb_pred_col: str = "y_pred",
        lgbm_pred_col: str = "y_pred_lgbm",
    ):
        self.xgb_oof_path = xgb_oof_path
        self.lgbm_oof_path = lgbm_oof_path
        self.target_col = target_col
        self.xgb_pred_col = xgb_pred_col
        self.lgbm_pred_col = lgbm_pred_col
    
    def load_oof_predictions(self) -> Tuple[pd.DataFrame, pd.DataFrame]:
        """Load OOF predictions from both models."""
        print("\nüìÇ Loading OOF predictions...")
        
        if not self.xgb_oof_path.exists():
            raise FileNotFoundError(
                f"XGBoost OOF file not found: {self.xgb_oof_path}\n"
                "Run the XGBoost CV to generate this file."
            )
        
        if not self.lgbm_oof_path.exists():
            raise FileNotFoundError(
                f"LightGBM OOF file not found: {self.lgbm_oof_path}\n"
                "Run: python scripts/run_phase4_lgbm_experiment.py"
            )
        
        df_xgb = pd.read_parquet(self.xgb_oof_path)
        df_lgbm = pd.read_parquet(self.lgbm_oof_path)
        
        print(f"  XGBoost OOF: {len(df_xgb):,} rows")
        print(f"  LightGBM OOF: {len(df_lgbm):,} rows")
        
        return df_xgb, df_lgbm
    
    def merge_oof_predictions(
        self,
        df_xgb: pd.DataFrame,
        df_lgbm: pd.DataFrame,
    ) -> pd.DataFrame:
        """Merge OOF predictions on index (row_id)."""
        print("\nüîó Merging predictions...")
        
        # Rename columns to avoid conflicts
        df_xgb = df_xgb.rename(columns={self.target_col: "y_true"})
        df_lgbm = df_lgbm.rename(columns={self.lgbm_pred_col: "y_pred_lgbm_"})
        
        # Merge on index
        df = df_xgb.join(
            df_lgbm[["y_pred_lgbm_"]],
            how="inner",
        )
        
        # Standardize column names
        df = df.rename(columns={
            self.xgb_pred_col: "y_pred_xgb",
            "y_pred_lgbm_": "y_pred_lgbm",
        })
        
        print(f"  Merged: {len(df):,} rows (inner join)")
        
        return df
    
    def evaluate_blend(
        self,
        df: pd.DataFrame,
        xgb_weight: float,
    ) -> EnsembleResult:
        """Evaluate a single blend weight."""
        lgbm_weight = 1.0 - xgb_weight
        
        y_true = df["y_true"]
        y_pred = xgb_weight * df["y_pred_xgb"] + lgbm_weight * df["y_pred_lgbm"]
        
        r2 = r2_score(y_true, y_pred)
        mae = mean_absolute_error(y_true, y_pred)
        rmse = np.sqrt(mean_squared_error(y_true, y_pred))
        
        return EnsembleResult(
            xgb_weight=xgb_weight,
            lgbm_weight=lgbm_weight,
            r2=r2,
            mae=mae,
            rmse=rmse,
        )
    
    def grid_search_blend(
        self,
        df: pd.DataFrame,
        weights: np.ndarray = BLEND_WEIGHTS,
    ) -> List[EnsembleResult]:
        """Grid search over blend weights."""
        print("\nüîç Grid searching blend weights...")
        
        results = []
        for w in weights:
            result = self.evaluate_blend(df, xgb_weight=w)
            results.append(result)
        
        print(f"  Evaluated {len(results)} weight combinations")
        
        return results
    
    def find_best_blend(
        self,
        results: List[EnsembleResult],
        metric: str = "mae",
    ) -> EnsembleResult:
        """Find best blend by specified metric."""
        if metric == "mae":
            best = min(results, key=lambda r: r.mae)
        elif metric == "rmse":
            best = min(results, key=lambda r: r.rmse)
        elif metric == "r2":
            best = max(results, key=lambda r: r.r2)
        else:
            raise ValueError(f"Unknown metric: {metric}")
        
        return best
    
    def evaluate_individual_models(
        self,
        df: pd.DataFrame,
    ) -> Tuple[EnsembleResult, EnsembleResult]:
        """Get metrics for individual models (100% weight)."""
        xgb_only = self.evaluate_blend(df, xgb_weight=1.0)
        lgbm_only = self.evaluate_blend(df, xgb_weight=0.0)
        return xgb_only, lgbm_only
    
    def run(self, save_path: Path = ENSEMBLE_METRICS_PATH) -> Dict[str, Any]:
        """Run full ensemble evaluation."""
        print("\n" + "=" * 70)
        print("üî¨ PHASE-4 ENSEMBLE EVALUATION")
        print("=" * 70)
        
        # Load predictions
        df_xgb, df_lgbm = self.load_oof_predictions()
        
        # Merge predictions
        df = self.merge_oof_predictions(df_xgb, df_lgbm)
        
        # Get individual model metrics
        xgb_result, lgbm_result = self.evaluate_individual_models(df)
        
        print("\nüìä INDIVIDUAL MODEL METRICS:")
        print("-" * 50)
        print(f"{'Model':<15} {'R¬≤':>10} {'MAE':>12} {'RMSE':>12}")
        print("-" * 50)
        print(f"{'XGBoost':<15} {xgb_result.r2:>10.4f} {xgb_result.mae:>12.2f} {xgb_result.rmse:>12.2f}")
        print(f"{'LightGBM':<15} {lgbm_result.r2:>10.4f} {lgbm_result.mae:>12.2f} {lgbm_result.rmse:>12.2f}")
        print("-" * 50)
        
        # Grid search
        results = self.grid_search_blend(df)
        
        # Find best by MAE
        best_by_mae = self.find_best_blend(results, metric="mae")
        best_by_r2 = self.find_best_blend(results, metric="r2")
        
        print("\nüèÜ BEST ENSEMBLE (by MAE):")
        print("-" * 50)
        print(f"  XGBoost weight: {best_by_mae.xgb_weight:.2f}")
        print(f"  LightGBM weight: {best_by_mae.lgbm_weight:.2f}")
        print(f"  R¬≤:  {best_by_mae.r2:.4f}")
        print(f"  MAE: {best_by_mae.mae:.2f}")
        print(f"  RMSE: {best_by_mae.rmse:.2f}")
        print("-" * 50)
        
        # Compare with individual models
        print("\nüìà IMPROVEMENT OVER INDIVIDUAL MODELS:")
        print("-" * 50)
        mae_improvement_xgb = (xgb_result.mae - best_by_mae.mae) / xgb_result.mae * 100
        mae_improvement_lgbm = (lgbm_result.mae - best_by_mae.mae) / lgbm_result.mae * 100
        print(f"  vs XGBoost:  {mae_improvement_xgb:+.2f}% MAE")
        print(f"  vs LightGBM: {mae_improvement_lgbm:+.2f}% MAE")
        print("-" * 50)
        
        # Save results
        summary = {
            "ensemble_type": "weighted_blend",
            "best_by_mae": {
                "xgb_weight": float(best_by_mae.xgb_weight),
                "lgbm_weight": float(best_by_mae.lgbm_weight),
                "r2": float(best_by_mae.r2),
                "mae": float(best_by_mae.mae),
                "rmse": float(best_by_mae.rmse),
            },
            "best_by_r2": {
                "xgb_weight": float(best_by_r2.xgb_weight),
                "lgbm_weight": float(best_by_r2.lgbm_weight),
                "r2": float(best_by_r2.r2),
                "mae": float(best_by_r2.mae),
                "rmse": float(best_by_r2.rmse),
            },
            "individual_models": {
                "xgb": {
                    "r2": float(xgb_result.r2),
                    "mae": float(xgb_result.mae),
                    "rmse": float(xgb_result.rmse),
                },
                "lgbm": {
                    "r2": float(lgbm_result.r2),
                    "mae": float(lgbm_result.mae),
                    "rmse": float(lgbm_result.rmse),
                },
            },
            "all_weights_evaluated": [
                {
                    "xgb_weight": float(r.xgb_weight),
                    "lgbm_weight": float(r.lgbm_weight),
                    "r2": float(r.r2),
                    "mae": float(r.mae),
                    "rmse": float(r.rmse),
                }
                for r in results
            ],
        }
        
        save_path.parent.mkdir(parents=True, exist_ok=True)
        with open(save_path, "w") as f:
            json.dump(summary, f, indent=2)
        print(f"\nüíæ Saved ensemble metrics to: {save_path}")
        
        print("\n" + "=" * 70)
        print("‚úÖ ENSEMBLE EVALUATION COMPLETE")
        print("=" * 70 + "\n")
        
        return summary


# =============================================================================
# Main
# =============================================================================

def main():
    evaluator = Phase4EnsembleEval()
    evaluator.run()


if __name__ == "__main__":
    main()
