{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb142e5",
   "metadata": {},
   "source": [
    "# B2_log_ctx_opt â€“ UIDAI Final Model (XGBoost)\n",
    "\n",
    "Goal:\n",
    "- Forecast UIDAI demand with high RÂ² and low MAE.\n",
    "- Ensure safety: at least 80% of state/segment groups have acceptable error.\n",
    "- Use a clean, reproducible pipeline:\n",
    "  data â†’ time split â†’ features â†’ XGBoost tuning â†’ evaluation â†’ safety."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8587bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Phase 1 setup complete\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROMPT FOR COPILOT â€“ Phase 1 (Path setup, imports, config)\n",
    "\n",
    "You are configuring the FINAL UIDAI notebook for the model B2_log_ctx_opt.\n",
    "\n",
    "Project structure:\n",
    "- root/\n",
    "    - src/\n",
    "        - uidai_features.py      # feature engineering module\n",
    "        - uidai_utils.py         # time-split, metrics, safety, save helpers\n",
    "    - notebooks/\n",
    "        - models/\n",
    "            - 06_B2_log_ctx_opt_final.ipynb   # THIS notebook\n",
    "    - data/\n",
    "    - artifacts/\n",
    "\n",
    "What this cell MUST do:\n",
    "\n",
    "1) Python path setup\n",
    "   - Add ../src to sys.path so that `uidai_features` and `uidai_utils`\n",
    "     can be imported when the notebook runs from notebooks/models/.\n",
    "\n",
    "2) Imports\n",
    "   - Core libraries: pandas, numpy, matplotlib, seaborn.\n",
    "   - Model: xgboost.XGBRegressor (regression).\n",
    "   - Project helpers:\n",
    "       from uidai_features import build_b2_log_ctx_opt_features\n",
    "       from uidai_utils import (\n",
    "           make_time_splits,\n",
    "           compute_regression_metrics,\n",
    "           compute_safety_report,\n",
    "           save_metrics_and_safety,\n",
    "       )\n",
    "\n",
    "3) Global configuration\n",
    "   - Define ROOT = project root (.. from this notebook).\n",
    "   - Define:\n",
    "       DATA_PATH   -> path to the main UIDAI monthly dataset\n",
    "       TARGET_COL  -> name of the target column\n",
    "       DATE_COL    -> name of the date column\n",
    "       OUTPUT_DIR  -> directory for artifacts of B2_log_ctx_opt\n",
    "       RANDOM_STATE -> fixed seed (e.g. 42) for reproducibility.\n",
    "\n",
    "4) Safety / feedback\n",
    "   - Print:\n",
    "       - the SRC_DIR being used\n",
    "       - a confirmation that imports worked\n",
    "       - the OUTPUT_DIR path\n",
    "   - If imports fail, DO NOT catch or hide the exception:\n",
    "     let Python raise the error so we can fix paths or filenames.\n",
    "\n",
    "Generate clean, readable Python code that follows these instructions.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Add ../src to Python path\n",
    "ROOT = Path(\"..\").resolve()\n",
    "SRC_DIR = ROOT / \"src\"\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SRC_DIR))\n",
    "\n",
    "print(\"Using SRC_DIR:\", SRC_DIR)\n",
    "\n",
    "# Project imports\n",
    "from uidai_features import build_b2_log_ctx_opt_features\n",
    "from uidai_utils import (\n",
    "    make_time_splits,\n",
    "    compute_regression_metrics,\n",
    "    compute_safety_report,\n",
    "    save_metrics_and_safety,\n",
    ")\n",
    "\n",
    "print(\"âœ… Imports OK: uidai_features and uidai_utils loaded.\")\n",
    "\n",
    "# Global config\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Data configuration (using actual UIDAI data paths)\n",
    "DATA_PATH = ROOT / \"data\" / \"processed\" / \"district_month_modeling.csv\"\n",
    "TARGET_COL = \"total_enrolment\"\n",
    "DATE_COL = \"month_date\"\n",
    "\n",
    "OUTPUT_DIR = ROOT / \"artifacts\" / \"B2_log_ctx_opt\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"OUTPUT_DIR:\", OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2aaafd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 2025-04-01 00:00:00 -> 2025-09-01 00:00:00 rows: 498\n",
      "Val  : 2025-10-01 00:00:00 -> 2025-10-01 00:00:00 rows: 962\n",
      "Test : 2025-11-01 00:00:00 -> 2025-12-01 00:00:00 rows: 1036\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 2 â€“ Load UIDAI data and create time-based splits for B2_log_ctx_opt.\n",
    "\n",
    "Context:\n",
    "- This notebook is the FINAL pipeline for B2_log_ctx_opt.\n",
    "- We are only improving this model; no new models are introduced here.\n",
    "- Copilot is helping generate boilerplate code, but all logic (paths, dates, columns) must be checked and corrected by us if needed.\n",
    "\n",
    "Tasks:\n",
    "1) Use the config defined in Phase 1:\n",
    "   - DATA_PATH: path to monthly UIDAI dataset (e.g. ../data/uidai_monthly/uidai_monthly.csv)\n",
    "   - TARGET_COL: true target column name\n",
    "   - DATE_COL: true date column name\n",
    "\n",
    "2) Load the data:\n",
    "   - Read CSV from DATA_PATH.\n",
    "   - Parse DATE_COL as datetime.\n",
    "   - Apply only LIGHT cleaning: type fixes, simple missing handling, obvious outliers.\n",
    "\n",
    "3) Create time-based splits (no random split):\n",
    "   - Choose real boundaries for train_end and val_end that match our previous experiments / hackathon rules.\n",
    "   - Call make_time_splits(df, DATE_COL, train_end=..., val_end=..., test_end=None or a final date).\n",
    "   - Ensure: train dates < val dates < test dates, no overlap.\n",
    "\n",
    "4) Sanity checks:\n",
    "   - Print head() and tail() of each split.\n",
    "   - Print date ranges and shapes for train_df, val_df, test_df.\n",
    "\"\"\"\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH, parse_dates=[DATE_COL])\n",
    "\n",
    "# Light cleaning: drop rows with missing target\n",
    "df = df.dropna(subset=[TARGET_COL])\n",
    "\n",
    "# Time splits: Train (Apr-Sep), Val (Oct), Test (Nov-Dec)\n",
    "TRAIN_END = \"2025-09-30\"\n",
    "VAL_END = \"2025-10-31\"\n",
    "TEST_END = None  # Use all remaining data as test\n",
    "\n",
    "train_df, val_df, test_df = make_time_splits(\n",
    "    df=df,\n",
    "    date_col=DATE_COL,\n",
    "    train_end=TRAIN_END,\n",
    "    val_end=VAL_END,\n",
    "    test_end=TEST_END,\n",
    ")\n",
    "\n",
    "print(\"Train:\", train_df[DATE_COL].min(), \"->\", train_df[DATE_COL].max(), \"rows:\", len(train_df))\n",
    "print(\"Val  :\", val_df[DATE_COL].min(), \"->\", val_df[DATE_COL].max(), \"rows:\", len(val_df))\n",
    "print(\"Test :\", test_df[DATE_COL].min(), \"->\", test_df[DATE_COL].max(), \"rows:\", len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6728d412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA_PATH: ..\\data\\processed\\district_month_modeling.csv\n",
      "TARGET_COL: total_enrolment\n",
      "DATE_COL : month_date\n",
      "build_b2_log_ctx_opt_features: <function build_b2_log_ctx_opt_features at 0x000001E7E09AD6C0>\n",
      "make_time_splits: <function make_time_splits at 0x000001E7E0FF6C00>\n",
      "Train: 2025-04-01 00:00:00 -> 2025-09-01 00:00:00 | rows: 498\n",
      "Val: 2025-10-01 00:00:00 -> 2025-10-01 00:00:00 | rows: 962\n",
      "Test: 2025-11-01 00:00:00 -> 2025-12-01 00:00:00 | rows: 1036\n",
      "âœ… Phase 1 & 2 configuration and time splits look consistent.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sanity checks for Phase 1 & Phase 2 (setup + time splits).\n",
    "\n",
    "Goal:\n",
    "- Confirm that our configuration and time-based splits are \"hackathon-ready\", not just basic scaffolding.\n",
    "- This cell should:\n",
    "  1) Print DATA_PATH, TARGET_COL, DATE_COL so we can visually confirm they are correct.\n",
    "  2) Verify that uidai_features and uidai_utils were imported successfully.\n",
    "  3) Check that train_df, val_df, test_df exist and are non-empty.\n",
    "  4) Print min/max dates and row counts for each split.\n",
    "  5) Assert that:\n",
    "        max(train dates) < min(val dates) <= max(val dates) < min(test dates)\n",
    "     to guarantee strictly ordered, non-overlapping time splits.\n",
    "If any assertion fails, we will fix the config/split logic before going to Phase 3.\n",
    "\"\"\"\n",
    "\n",
    "# 1) Show core config\n",
    "print(\"DATA_PATH:\", DATA_PATH)\n",
    "print(\"TARGET_COL:\", TARGET_COL)\n",
    "print(\"DATE_COL :\", DATE_COL)\n",
    "\n",
    "# 2) Basic import confirmation (types)\n",
    "print(\"build_b2_log_ctx_opt_features:\", build_b2_log_ctx_opt_features)\n",
    "print(\"make_time_splits:\", make_time_splits)\n",
    "\n",
    "# 3) Check that splits exist and are non-empty\n",
    "for name, df_part in [(\"train_df\", train_df), (\"val_df\", val_df), (\"test_df\", test_df)]:\n",
    "    assert df_part is not None, f\"{name} is None\"\n",
    "    assert len(df_part) > 0, f\"{name} is empty\"\n",
    "\n",
    "# 4) Print date ranges and row counts\n",
    "for name, df_part in [(\"Train\", train_df), (\"Val\", val_df), (\"Test\", test_df)]:\n",
    "    print(\n",
    "        f\"{name}: {df_part[DATE_COL].min()} -> {df_part[DATE_COL].max()} | rows: {len(df_part)}\"\n",
    "    )\n",
    "\n",
    "# 5) Assert ordering of splits\n",
    "train_max = train_df[DATE_COL].max()\n",
    "val_min = val_df[DATE_COL].min()\n",
    "val_max = val_df[DATE_COL].max()\n",
    "test_min = test_df[DATE_COL].min()\n",
    "\n",
    "assert train_max < val_min, \"Train and validation date ranges overlap or are not ordered\"\n",
    "assert val_max < test_min, \"Validation and test date ranges overlap or are not ordered\"\n",
    "\n",
    "print(\"âœ… Phase 1 & 2 configuration and time splits look consistent.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d761bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (486, 31) y_train: (486,)\n",
      "X_val  : (950, 31) y_val  : (950,)\n",
      "X_test : (1024, 31) y_test : (1024,)\n",
      "\n",
      "Feature columns: ['state', 'district', 'year_month', 'age_0_5', 'age_5_17', 'age_18_greater', 'demo_age_5_17', 'demo_age_17_', 'bio_age_5_17', 'bio_age_17_'] ...\n",
      "âœ… Phase 3 features ready for XGBoost B2_log_ctx_opt.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "PROMPT FOR COPILOT â€“ Phase 3 (Final features for B2_log_ctx_opt)\n",
    "\n",
    "Context:\n",
    "- Phase 1 and Phase 2 are passing (imports + time-based splits).\n",
    "- We now have: train_df, val_df, test_df with correct date ranges.\n",
    "- All feature logic must live in build_b2_log_ctx_opt_features in uidai_features.py.\n",
    "- We are NOT creating a new model, only improving B2_log_ctx_opt.\n",
    "\n",
    "Tasks for this cell:\n",
    "1) Call build_b2_log_ctx_opt_features on each split:\n",
    "   - (X_train, y_train) from train_df\n",
    "   - (X_val,   y_val)   from val_df\n",
    "   - (X_test,  y_test)  from test_df\n",
    "\n",
    "2) Print shapes of all X_*/y_* for a quick overview.\n",
    "\n",
    "3) Run sanity checks:\n",
    "   - len(X_*) == len(y_*) for train/val/test\n",
    "   - No NaNs in X_train, X_val, X_test\n",
    "   - Optionally print first few feature columns so we see time / policy / segment features.\n",
    "\n",
    "If any check fails, we will fix build_b2_log_ctx_opt_features in src/uidai_features.py and rerun.\n",
    "\"\"\"\n",
    "\n",
    "X_train, y_train = build_b2_log_ctx_opt_features(train_df, TARGET_COL, DATE_COL)\n",
    "X_val, y_val = build_b2_log_ctx_opt_features(val_df, TARGET_COL, DATE_COL)\n",
    "X_test, y_test = build_b2_log_ctx_opt_features(test_df, TARGET_COL, DATE_COL)\n",
    "\n",
    "print(\"X_train:\", X_train.shape, \"y_train:\", y_train.shape)\n",
    "print(\"X_val  :\", X_val.shape, \"y_val  :\", y_val.shape)\n",
    "print(\"X_test :\", X_test.shape, \"y_test :\", y_test.shape)\n",
    "\n",
    "for name, X, y in [\n",
    "    (\"train\", X_train, y_train),\n",
    "    (\"val\", X_val, y_val),\n",
    "    (\"test\", X_test, y_test),\n",
    "]:\n",
    "    assert len(X) == len(y), f\"Length mismatch in {name} set\"\n",
    "    assert not pd.isna(X).any().any(), f\"NaNs found in X_{name}\"\n",
    "\n",
    "print(\"\\nFeature columns:\", list(X_train.columns[:10]), \"...\")\n",
    "print(\"âœ… Phase 3 features ready for XGBoost B2_log_ctx_opt.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38116588",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded categorical columns: ['state', 'district', 'year_month', 'volume_bucket']\n",
      "X_train_enc dtypes: {dtype('float64'): 22, dtype('int32'): 5, dtype('int64'): 4}\n",
      "\n",
      "Testing 30 hyperparameter combinations...\n",
      "  [10/30] Current best MAE: 109.60\n",
      "  [20/30] Current best MAE: 108.28\n",
      "  [30/30] Current best MAE: 108.28\n",
      "\n",
      "============================================================\n",
      "BEST HYPERPARAMETERS:\n",
      "  n_estimators: 500\n",
      "  max_depth: 3\n",
      "  learning_rate: 0.03\n",
      "  subsample: 0.8\n",
      "  colsample_bytree: 1.0\n",
      "  reg_lambda: 1.0\n",
      "  reg_alpha: 0.5\n",
      "\n",
      "TRAIN METRICS:\n",
      "  MAE:  24.24\n",
      "  RMSE: 32.44\n",
      "  MAPE: 2854.10%\n",
      "  RÂ²:   0.9997\n",
      "\n",
      "VALIDATION METRICS:\n",
      "  MAE:  108.28\n",
      "  RMSE: 190.03\n",
      "  MAPE: 5842.57%\n",
      "  RÂ²:   0.9654\n",
      "\n",
      "âœ… Phase 4 complete â€“ best_model ready for test evaluation.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 4 â€“ Train and tune XGBoost (B2_log_ctx_opt).\n",
    "\n",
    "Goal:\n",
    "Train and tune the EXISTING B2_log_ctx_opt model using XGBoost on the Phase 3 features,\n",
    "without creating any new model family. We want:\n",
    "- Higher RÂ², lower MAE/RMSE/MAPE on the validation split.\n",
    "- A clean, simple training cell that judges can read.\n",
    "- No data leakage (respect the time-based split from Phase 2).\n",
    "\n",
    "Approach:\n",
    "- Manual hyperparameter loop (fit on train, eval on val) â€“ explicit and judge-friendly.\n",
    "- Sample up to 30 combinations to keep runtime reasonable.\n",
    "- Encode categorical columns to numeric for XGBoost compatibility.\n",
    "\"\"\"\n",
    "\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# â”€â”€ Encode categorical columns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cat_cols = [\"state\", \"district\", \"year_month\", \"volume_bucket\"]\n",
    "encoders = {}\n",
    "\n",
    "X_train_enc = X_train.copy()\n",
    "X_val_enc = X_val.copy()\n",
    "X_test_enc = X_test.copy()\n",
    "\n",
    "for col in cat_cols:\n",
    "    if col in X_train_enc.columns:\n",
    "        le = LabelEncoder()\n",
    "        # Fit on all data to handle unseen categories in val/test\n",
    "        all_values = pd.concat([X_train_enc[col].astype(str), \n",
    "                                 X_val_enc[col].astype(str), \n",
    "                                 X_test_enc[col].astype(str)]).unique()\n",
    "        le.fit(all_values)\n",
    "        \n",
    "        X_train_enc[col] = le.transform(X_train_enc[col].astype(str))\n",
    "        X_val_enc[col] = le.transform(X_val_enc[col].astype(str))\n",
    "        X_test_enc[col] = le.transform(X_test_enc[col].astype(str))\n",
    "        encoders[col] = le\n",
    "\n",
    "print(\"Encoded categorical columns:\", list(encoders.keys()))\n",
    "print(\"X_train_enc dtypes:\", X_train_enc.dtypes.value_counts().to_dict())\n",
    "\n",
    "# â”€â”€ Define hyperparameter search space â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "param_grid = {\n",
    "    \"n_estimators\": [300, 500, 700],\n",
    "    \"max_depth\": [3, 4, 5],\n",
    "    \"learning_rate\": [0.03, 0.06],\n",
    "    \"subsample\": [0.8, 1.0],\n",
    "    \"colsample_bytree\": [0.8, 1.0],\n",
    "    \"reg_lambda\": [1.0, 5.0],\n",
    "    \"reg_alpha\": [0.0, 0.5],\n",
    "}\n",
    "\n",
    "# Generate all combinations and sample 30\n",
    "all_combos = [\n",
    "    {\n",
    "        \"n_estimators\": n,\n",
    "        \"max_depth\": d,\n",
    "        \"learning_rate\": lr,\n",
    "        \"subsample\": ss,\n",
    "        \"colsample_bytree\": cs,\n",
    "        \"reg_lambda\": rl,\n",
    "        \"reg_alpha\": ra,\n",
    "    }\n",
    "    for n in param_grid[\"n_estimators\"]\n",
    "    for d in param_grid[\"max_depth\"]\n",
    "    for lr in param_grid[\"learning_rate\"]\n",
    "    for ss in param_grid[\"subsample\"]\n",
    "    for cs in param_grid[\"colsample_bytree\"]\n",
    "    for rl in param_grid[\"reg_lambda\"]\n",
    "    for ra in param_grid[\"reg_alpha\"]\n",
    "]\n",
    "\n",
    "random.seed(RANDOM_STATE)\n",
    "sampled_combos = random.sample(all_combos, min(30, len(all_combos)))\n",
    "print(f\"\\nTesting {len(sampled_combos)} hyperparameter combinations...\")\n",
    "\n",
    "# â”€â”€ Manual tuning loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "best_params = None\n",
    "best_val_mae = float(\"inf\")\n",
    "best_model = None\n",
    "results = []\n",
    "\n",
    "for i, params in enumerate(sampled_combos, 1):\n",
    "    full_params = {\n",
    "        **params,\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"random_state\": RANDOM_STATE,\n",
    "        \"tree_method\": \"hist\",\n",
    "    }\n",
    "    \n",
    "    model = XGBRegressor(**full_params)\n",
    "    model.fit(X_train_enc, y_train)\n",
    "    \n",
    "    y_pred_val = model.predict(X_val_enc)\n",
    "    val_metrics = compute_regression_metrics(y_val, y_pred_val)\n",
    "    val_mae = val_metrics[\"mae\"]\n",
    "    \n",
    "    results.append({**params, \"val_mae\": val_mae, \"val_r2\": val_metrics[\"r2\"]})\n",
    "    \n",
    "    if val_mae < best_val_mae:\n",
    "        best_val_mae = val_mae\n",
    "        best_params = full_params\n",
    "        best_model = model\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"  [{i}/{len(sampled_combos)}] Current best MAE: {best_val_mae:.2f}\")\n",
    "\n",
    "# â”€â”€ Report best configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BEST HYPERPARAMETERS:\")\n",
    "for k, v in best_params.items():\n",
    "    if k not in [\"objective\", \"random_state\", \"tree_method\"]:\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "# Recompute metrics for the best model\n",
    "y_pred_train = best_model.predict(X_train_enc)\n",
    "y_pred_val = best_model.predict(X_val_enc)\n",
    "\n",
    "train_metrics = compute_regression_metrics(y_train, y_pred_train)\n",
    "val_metrics = compute_regression_metrics(y_val, y_pred_val)\n",
    "\n",
    "print(\"\\nTRAIN METRICS:\")\n",
    "print(f\"  MAE:  {train_metrics['mae']:.2f}\")\n",
    "print(f\"  RMSE: {train_metrics['rmse']:.2f}\")\n",
    "print(f\"  MAPE: {train_metrics['mape']:.2%}\")\n",
    "print(f\"  RÂ²:   {train_metrics['r2']:.4f}\")\n",
    "\n",
    "print(\"\\nVALIDATION METRICS:\")\n",
    "print(f\"  MAE:  {val_metrics['mae']:.2f}\")\n",
    "print(f\"  RMSE: {val_metrics['rmse']:.2f}\")\n",
    "print(f\"  MAPE: {val_metrics['mape']:.2%}\")\n",
    "print(f\"  RÂ²:   {val_metrics['r2']:.4f}\")\n",
    "\n",
    "print(\"\\nâœ… Phase 4 complete â€“ best_model ready for test evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79ce264c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "B2_log_ctx_opt â€“ Phase 4 Tuning Summary (XGBoost)\n",
      "============================================================\n",
      "\n",
      "ðŸ“Š Best Validation MAE: 108.28\n",
      "\n",
      "ðŸ”§ Best Hyperparameters:\n",
      "----------------------------------------\n",
      "  n_estimators        : 500\n",
      "  max_depth           : 3\n",
      "  learning_rate       : 0.03\n",
      "  subsample           : 0.8\n",
      "  colsample_bytree    : 1.0\n",
      "  reg_lambda          : 1.0\n",
      "  reg_alpha           : 0.5\n",
      "\n",
      "ðŸ“ˆ Model Performance:\n",
      "----------------------------------------\n",
      "Metric            Train   Validation\n",
      "----------------------------------------\n",
      "MAE               24.24       108.28\n",
      "RMSE              32.44       190.03\n",
      "MAPE          2854.10%    5842.57%\n",
      "R2               0.9997       0.9654\n",
      "\n",
      "âœ… Phase 4 complete â€“ best_model, best_params, and metrics are ready for Phase 5/6.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Phase 4 Summary â€“ Tuning Results Report\n",
    "\n",
    "Context:\n",
    "- The previous cell performed a manual hyperparameter search over XGBoost\n",
    "  and created:\n",
    "    - best_model    -> fitted XGBRegressor with best params\n",
    "    - best_params   -> dict of best hyperparameters\n",
    "    - best_val_mae  -> best validation MAE value\n",
    "    - train_metrics -> regression metrics on (X_train, y_train)\n",
    "    - val_metrics   -> regression metrics on (X_val, y_val)\n",
    "\n",
    "Goal of THIS cell:\n",
    "- Print a clean, judge-friendly summary of tuning results.\n",
    "- Do NOT retrain the model or rerun the whole search; just report results.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"B2_log_ctx_opt â€“ Phase 4 Tuning Summary (XGBoost)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ“Š Best Validation MAE: {best_val_mae:.2f}\")\n",
    "\n",
    "print(\"\\nðŸ”§ Best Hyperparameters:\")\n",
    "print(\"-\" * 40)\n",
    "for k, v in best_params.items():\n",
    "    if k not in [\"objective\", \"random_state\", \"tree_method\"]:\n",
    "        print(f\"  {k:20s}: {v}\")\n",
    "\n",
    "print(\"\\nðŸ“ˆ Model Performance:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Metric':<10} {'Train':>12} {'Validation':>12}\")\n",
    "print(\"-\" * 40)\n",
    "for metric in [\"mae\", \"rmse\", \"mape\", \"r2\"]:\n",
    "    train_val = train_metrics[metric]\n",
    "    val_val = val_metrics[metric]\n",
    "    if metric == \"mape\":\n",
    "        print(f\"{metric.upper():<10} {train_val:>11.2%} {val_val:>11.2%}\")\n",
    "    elif metric == \"r2\":\n",
    "        print(f\"{metric.upper():<10} {train_val:>12.4f} {val_val:>12.4f}\")\n",
    "    else:\n",
    "        print(f\"{metric.upper():<10} {train_val:>12.2f} {val_val:>12.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Phase 4 complete â€“ best_model, best_params, and metrics are ready for Phase 5/6.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b2ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Phase 5 & 6 â€“ Final evaluation and safety.\n",
    "\n",
    "Steps:\n",
    "- Fit best XGBoost on train+val\n",
    "- Evaluate on test (metrics)\n",
    "- Compute safety report by state group / volume bucket\n",
    "\"\"\"\n",
    "\n",
    "# TODO: after hyperparameter search:\n",
    "# best_params = search.best_params_\n",
    "# best_model = XGBRegressor(**best_params, objective=\"reg:squarederror\", random_state=RANDOM_STATE)\n",
    "\n",
    "# TODO: fit best_model on X_train, y_train (or train+val)\n",
    "# y_pred_train = best_model.predict(X_train)\n",
    "# y_pred_val = best_model.predict(X_val)\n",
    "# y_pred_test = best_model.predict(X_test)\n",
    "\n",
    "metrics = {\n",
    "    \"train\": compute_regression_metrics(y_train, y_pred_train),\n",
    "    \"val\": compute_regression_metrics(y_val, y_pred_val),\n",
    "    \"test\": compute_regression_metrics(y_test, y_pred_test),\n",
    "}\n",
    "\n",
    "# Example grouping: state and volume_bucket columns must exist in test_df\n",
    "safety_df, safety_summary = compute_safety_report(\n",
    "    y_true=y_test,\n",
    "    y_pred=y_pred_test,\n",
    "    groups_df=test_df[[\"state\", \"volume_bucket\"]],\n",
    "    mae_factor_threshold=1.5,\n",
    "    mape_threshold=None,\n",
    ")\n",
    "\n",
    "save_metrics_and_safety(metrics, safety_df, OUTPUT_DIR)\n",
    "\n",
    "metrics, safety_summary"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
